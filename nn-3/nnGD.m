function [L_history, thetah2o, thetah1h2, thetaih1] = nnGD(X, Y, thetaih1, thetah1h2, thetah2o, lambda, niter, alpha)    nTrain = length(Y)    L_history = zeros(niter, 1)        for i = 1:niter                [L, gradh2o, gradh1h2, gradih1] = nnCostFunctionSig(X, Y, thetaih1, thetah1h2, thetah2o, lambda)        L_history(i) = L% get loss function for training and validation data set for each iteration                thetah2o = thetah2o - alpha * gradh2o        thetah1h2 = thetah1h2 - alpha * gradh1h2        thetaih1 = thetaih1 - alpha * gradih1 % update weights based on gradient    end        end